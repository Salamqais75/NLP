Natural Language Processing â€“ University of Haifa, Semester B 2024

Part 1: Tokenization and Word2Vec Model Training

-Tokenization: The text from the Knesset protocols is preprocessed by splitting it into individual sentences and then into tokens (words). This step is crucial for preparing the data for further analysis and training.

-Word2Vec Model Training: A Word2Vec model is trained on the tokenized text to learn word embeddings, which represent words in a continuous vector space based on their context. The embeddings capture semantic relationships between words, enabling more sophisticated text analysis. The model parameters vector_size, window, and min_count were carefully selected and justified in the report.

Part 2: Language Modeling with Trigrams

-Generating the Most Likely Next Token: Given a sequence of two tokens, the model predicts the most likely third token based on trigram probabilities.

-Calculating Log Probability of Sentences: Using either Laplace smoothing or Linear interpolation, the model computes the log probability of a given sentence. This helps in determining the likelihood of a sentence being generated by the model.

-Analyzing N-Gram Collocations: The model analyzes collocations (frequently co-occurring words) using trigrams, providing insights into common phrases or terms in the Knesset corpus.

-Filling in Missing Tokens: The model attempts to fill in missing tokens in sentences based on trigram probabilities, useful for tasks like autocompletion or text correction.

-Model Comparison: This task involves comparing two trigram models (committee and plenary) to determine which model is more likely to generate a given sentence, aiding in categorizing text based on its origin.

Part 3: Classification with Knesset Protocol Corpus

-Defining Classes and Chunking Sentences: The corpus is divided into classes, and sentences are chunked into smaller segments for better classification performance.

-Creating Feature Vectors: Custom features, including TF-IDF vectors, are engineered to numerically represent the text data for classification.

-Training Classifiers: Multiple classifiers, including K-Nearest Neighbors (KNN) and Logistic Regression, are trained using the feature vectors. These models are evaluated using 5-fold cross-validation and a test-train split.

-Classifying New Data: The trained models are used to classify new, unseen data, demonstrating their ability to generalize beyond the training set.

-Experimenting with Chunk Sizes: Various chunk sizes are tested to determine their impact on classification performance, optimizing the model.

-Evaluation and Reporting: The final step involves analyzing the model's performance, addressing specific questions about model evaluation methods, and documenting the findings in a detailed report.


Part 4: Word Embeddings

-This assignment involves working with word embeddings to explore word similarity, language models, and classification tasks using the Knesset corpus.

-Training a Word2Vec Model
We train a Word2Vec model using the Knesset corpus to create word embeddings for each word. The model is trained using tokenized sentences, removing non-word tokens. The parameters vector_size, window, and min_count were carefully selected and justified in the report.


-Word Similarity
We explore word similarity by identifying the closest words to a set of given words using cosine similarity. Sentence embeddings are also created, and we analyze sentence similarity. Finally, we replace words in selected sentences with similar words to maintain their meaning.


-Classification Using Sentence Embeddings
A KNN classifier is trained using sentence embeddings to classify Knesset text chunks as committee or plenary. The results are compared with those obtained using custom features from a previous assignment.


- Use of Large Language Models
We briefly explore the application of large language models for improving word embeddings and classification tasks, comparing the results with those from traditional methods.
